1、storage/innobase/handler/ha_innodb.cc
+/** Possible values of the parameter innodb_lock_schedule_algorithm */
+static const char* innodb_lock_schedule_algorithm_names[] = {
+	"fcfs",
+	"vats",
+	NullS
+};
+
+/** Used to define an enumerate type of the system variable
+innodb_lock_schedule_algorithm. */
+static TYPELIB innodb_lock_schedule_algorithm_typelib = {
+	array_elements(innodb_lock_schedule_algorithm_names) - 1,
+	"innodb_lock_schedule_algorithm_typelib",
+	innodb_lock_schedule_algorithm_names,
+	NULL
+};
+
 /* The following counter is used to convey information to InnoDB
 about server activity: in case of normal DML ops it is not
 sensible to call srv_active_wake_master_thread after each
 @@ -19467,6 +19483,18 @@ static MYSQL_SYSVAR_ULONG(doublewrite_batch_size, srv_doublewrite_batch_size,
   NULL, NULL, 120, 1, 127, 0);
 #endif /* defined UNIV_DEBUG || defined UNIV_PERF_DEBUG */
 
+static MYSQL_SYSVAR_ENUM(lock_schedule_algorithm, innodb_lock_schedule_algorithm,
+  PLUGIN_VAR_RQCMDARG,
+  "The algorithm Innodb uses for deciding which locks to grant next when"
+  " a lock is released. Possible values are"
+  " FCFS"
+  " grant the locks in First-Come-First-Served order;"
+  " VATS"
+  " use the Variance-Aware-Transaction-Scheduling algorithm, which"
+  " uses an Eldest-Transaction-First heuristic.",
+  NULL, NULL, INNODB_LOCK_SCHEDULE_ALGORITHM_FCFS,
+  &innodb_lock_schedule_algorithm_typelib);
+
 static MYSQL_SYSVAR_ULONG(buffer_pool_instances, srv_buf_pool_instances,
   PLUGIN_VAR_RQCMDARG | PLUGIN_VAR_READONLY,
   "Number of buffer pool instances, set to higher value on high-end machines to increase scalability",
 @@ -20069,7 +20097,8 @@ static struct st_mysql_sys_var* innobase_system_variables[]= {
   MYSQL_SYSVAR(ft_num_word_optimize),
   MYSQL_SYSVAR(ft_sort_pll_degree),
   MYSQL_SYSVAR(large_prefix),
-  MYSQL_SYSVAR(force_load_corrupted),
+	MYSQL_SYSVAR(force_load_corrupted),
+	MYSQL_SYSVAR(lock_schedule_algorithm),
   MYSQL_SYSVAR(locks_unsafe_for_binlog),
   MYSQL_SYSVAR(lock_wait_timeout),
   MYSQL_SYSVAR(deadlock_detect),

2、storage/innobase/include/lock0lock.h
 extern my_bool	innobase_deadlock_detect;
 
+/** Alternatives for innodb_lock_schedule_algorithm, which can be changed by
+ setting innodb_lock_schedule_algorithm. */
+enum innodb_lock_schedule_algorithm_t {
+	/*!< First Come First Served */
+	INNODB_LOCK_SCHEDULE_ALGORITHM_FCFS,
+	/*!< Variance-Aware-Transaction-Scheduling */
+	INNODB_LOCK_SCHEDULE_ALGORITHM_VATS
+};
+
+extern ulong innodb_lock_schedule_algorithm;
+
 /*********************************************************************//**
 Gets the size of a lock struct.
 @return size in bytes */

3、storage/innobase/include/trx0trx.h
@@ -1089,6 +1089,8 @@ struct trx_t {
 
 	time_t		start_time;	/*!< time the state last time became
 					TRX_STATE_ACTIVE */
+	long			dep_size;
+	bool			size_updated;
 	lsn_t		commit_lsn;	/*!< lsn at the time of the commit */
 	table_id_t	table_id;	/*!< Table to drop iff dict_operation
 					== TRX_DICT_OP_TABLE, or 0. */

4、storage/innobase/lock/lock0lock.cc
@@ -49,11 +49,16 @@ Created 5/7/1996 Heikki Tuuri
 #include "row0mysql.h"
 #include "pars0pars.h"
 
+#include <algorithm>
 #include <set>
+#include <vector>
 
 /* Flag to enable/disable deadlock detector. */
 my_bool	innobase_deadlock_detect = TRUE;
 
+/** Lock scheduling algorithm */
+ulong innodb_lock_schedule_algorithm = INNODB_LOCK_SCHEDULE_ALGORITHM_VATS;
+
 /** Total number of cached record locks */
 static const ulint	REC_LOCK_CACHE = 8;
 
 @@ -1488,6 +1493,196 @@ RecLock::lock_alloc(
 	return(lock);
 }
 
+/*********************************************************************//**
+Check if lock1 has higher priority than lock2.
+NULL has lowest priority.
+If either is a high priority transaction, the lock has higher priority.
+If neither of them is wait lock, the first one has higher priority.
+If only one of them is a wait lock, it has lower priority.
+Otherwise, the one with an older transaction has higher priority.
+@returns true if lock1 has higher priority, false otherwise. */
+static
+bool
+has_higher_priority(
+	lock_t *lock1,
+	lock_t *lock2)
+{
+	if (lock1 == NULL) {
+		return false;
+	} else if (lock2 == NULL) {
+		return true;
+	}
+	if (!lock_get_wait(lock1)) {
+		return true;
+	} else if (!lock_get_wait(lock2)) {
+		return false;
+	}
+	if (trx_is_high_priority(lock1->trx)) {
+		return true;
+	}
+	if (trx_is_high_priority(lock2->trx)) {
+		return false;
+	}
+	return lock1->trx->dep_size > lock2->trx->dep_size;
+}
+
+static
+bool
+use_vats(
+    trx_t *trx)
+{
+    return innodb_lock_schedule_algorithm ==
+		   INNODB_LOCK_SCHEDULE_ALGORITHM_VATS
+		&& !thd_is_replication_slave_thread(trx->mysql_thd);
+}
+
+static
+lock_t *
+lock_rec_get_first(
+    hash_table_t *hash,
+    ulint   space,
+    ulint   page_no,
+    ulint   heap_no)
+{
+	lock_t *lock;
+
+	lock = lock_rec_get_first_on_page_addr(hash, space, page_no);
+	if (lock != NULL && !lock_rec_get_nth_bit(lock, heap_no)) {
+		lock = lock_rec_get_next(heap_no, lock);
+	}
+
+	return lock;
+}
+
+static
+void
+lock_rec_insert_to_head(
+	hash_table_t *lock_hash,
+    lock_t *lock,
+    ulint   rec_fold)
+{
+	lock_t *next;
+	hash_cell_t* cell;
+
+	// Move the target lock to the head of the list
+	cell = hash_get_nth_cell(lock_hash, hash_calc_hash(rec_fold, lock_hash));
+	if (lock != cell->node) {
+		next = (lock_t *) cell->node;
+		cell->node = lock;
+		lock->hash = next;
+	}
+}
+
+static
+void
+reset_trx_size_updated()
+{
+	trx_t *trx;
+	for (trx = UT_LIST_GET_FIRST(trx_sys->rw_trx_list);
+		 trx != NULL;
+		 trx = UT_LIST_GET_NEXT(trx_list, trx)) {
+		trx->size_updated = false;
+	}
+	for (trx = UT_LIST_GET_FIRST(trx_sys->mysql_trx_list);
+		 trx != NULL;
+		 trx = UT_LIST_GET_NEXT(trx_list, trx)) {
+		trx->size_updated = false;
+	}
+}
+
+static
+void
+update_dep_size(
+    trx_t  *trx,
+    long    size_delta,
+    long    depth=1)
+{
+	ulint   space;
+	ulint   page_no;
+	ulint   heap_no;
+	lock_t *lock;
+	lock_t *wait_lock;
+	hash_table_t *lock_hash;
+
+	if (!use_vats(trx) || trx->size_updated || size_delta == 0) {
+		return;
+	}
+
+	trx->size_updated = true;
+	trx->dep_size += size_delta;
+	if (trx->dep_size < 0) {
+		trx->dep_size = 0;
+	}
+	wait_lock = trx->lock.wait_lock;
+	if (trx->state != TRX_STATE_ACTIVE
+		|| wait_lock == NULL) {
+		if (depth == 1) {
+			reset_trx_size_updated();
+		}
+		return;
+	}
+
+	space = wait_lock->un_member.rec_lock.space;
+	page_no = wait_lock->un_member.rec_lock.page_no;
+	heap_no = lock_rec_find_set_bit(wait_lock);
+	lock_hash = lock_hash_get(wait_lock->type_mode);
+	for (lock = lock_rec_get_first(lock_hash, space, page_no, heap_no);
+		 lock != NULL;
+		 lock = lock_rec_get_next(heap_no, lock)) {
+		if (!lock_get_wait(lock)
+			&& trx != lock->trx) {
+			update_dep_size(lock->trx, size_delta, depth + 1);
+		}
+	}
+	if (depth == 1) {
+		reset_trx_size_updated();
+	}
+}
+
+static
+void
+update_dep_size(
+    lock_t *in_lock,
+    ulint   heap_no,
+    bool    wait)
+{
+	lock_t *lock;
+	ulint   space;
+	ulint   page_no;
+	long    total_size_delta;
+	hash_table_t *lock_hash;
+
+	if (!use_vats(in_lock->trx)) {
+		return;
+	}
+
+	space = in_lock->un_member.rec_lock.space;
+	page_no = in_lock->un_member.rec_lock.page_no;
+	lock_hash = lock_hash_get(in_lock->type_mode);
+
+	if (wait) {
+		for (lock = lock_rec_get_first(lock_hash, space, page_no, heap_no);
+			 lock != NULL;
+			 lock = lock_rec_get_next(heap_no, lock)) {
+			if (!lock_get_wait(lock)
+				&& in_lock->trx != lock->trx) {
+				update_dep_size(lock->trx, in_lock->trx->dep_size + 1);
+			}
+		}
+	} else {
+		total_size_delta = 0;
+		for (lock = lock_rec_get_first(lock_hash, space, page_no, heap_no);
+			 lock != NULL;
+			 lock = lock_rec_get_next(heap_no, lock)) {
+			if (lock_get_wait(lock)
+				&& in_lock->trx != lock->trx) {
+				total_size_delta += lock->trx->dep_size + 1;
+			}
+		}
+		update_dep_size(in_lock->trx, total_size_delta);
+	}
+}
+
 /**
 Add the lock to the record lock hash and the transaction's lock list
 @param[in,out] lock	Newly created record lock to add to the rec hash
 @@ -1498,19 +1693,28 @@ RecLock::lock_add(lock_t* lock, bool add_to_hash)
 	ut_ad(lock_mutex_own());
 	ut_ad(trx_mutex_own(lock->trx));
 
+	bool wait = m_mode & LOCK_WAIT;
+
 	if (add_to_hash) {
 		ulint	key = m_rec_id.fold();
+		hash_table_t *lock_hash = lock_hash_get(m_mode);
 
 		++lock->index->table->n_rec_locks;
 
-		HASH_INSERT(lock_t, hash, lock_hash_get(m_mode), key, lock);
+		if (use_vats(lock->trx) && !wait) {
+			lock_rec_insert_to_head(lock_hash, lock, key);
+		} else {
+			HASH_INSERT(lock_t, hash, lock_hash, key, lock);
+		}
 	}
 
-	if (m_mode & LOCK_WAIT) {
+	UT_LIST_ADD_LAST(lock->trx->lock.trx_locks, lock);
+
+	if (wait) {
 		lock_set_lock_and_trx_wait(lock, lock->trx);
+	} else {
+		update_dep_size(lock, lock_rec_find_set_bit(lock), false);
 	}
-
-	UT_LIST_ADD_LAST(lock->trx->lock.trx_locks, lock);
 }
 
 /**
 @@ -1733,6 +1937,8 @@ RecLock::add_to_waitq(const lock_t* wait_for, const lock_prdt_t* prdt)
 
 	ut_ad(trx_mutex_own(m_trx));
 
+	update_dep_size(lock, lock_rec_find_set_bit(lock), err == DB_LOCK_WAIT || err == DB_DEADLOCK);
+
 	/* m_trx->mysql_thd is NULL if it's an internal trx. So current_thd is used */
 	if (err == DB_LOCK_WAIT) {
 		thd_report_row_lock_wait(current_thd, wait_for->trx->mysql_thd);
 @@ -1833,6 +2039,7 @@ lock_rec_add_to_queue(
 		if (lock != NULL) {
 
 			lock_rec_set_nth_bit(lock, heap_no);
+			update_dep_size(lock, heap_no, false);
 
 			return;
 		}
 @@ -1916,6 +2123,7 @@ lock_rec_lock_fast(
 			if (!lock_rec_get_nth_bit(lock, heap_no)) {
 				lock_rec_set_nth_bit(lock, heap_no);
 				status = LOCK_REC_SUCCESS_CREATED;
+				update_dep_size(lock, heap_no, false);
 			}
 		}
 
 @@ -2423,6 +2631,111 @@ lock_rec_cancel(
 	trx_mutex_exit(lock->trx);
 }
 
+/*********************************************************************//**
+Checks if a waiting record lock request still has to for granted locks.
+@return	lock that is causing the wait */
+static
+const lock_t*
+lock_rec_has_to_wait_granted(
+/*==========================*/
+	const lock_t*	wait_lock,	/*!< in: waiting record lock */
+    std::vector<lock_t *>   &granted_locks)  /*!< in: granted record lock */
+{
+	ulint   i;
+	lock_t *lock;
+	for (i = 0; i < granted_locks.size(); ++i) {
+		lock = granted_locks[i];
+		if (lock_has_to_wait(wait_lock, lock)) {
+			return lock;
+		}
+	}
+	return NULL;
+}
+
+static
+void
+vats_grant(
+	hash_table_t *lock_hash,
+    lock_t *released_lock,
+    ulint   heap_no)
+{
+	ulint			space;
+	ulint			page_no;
+	ulint			rec_fold;
+	ulint     i;
+	ulint     j;
+	long      sub_dep_size_total;
+	long      add_dep_size_total;
+	long      dep_size_compsensate;
+	lock_t*		lock;
+	lock_t*		wait_lock;
+	lock_t*		new_granted_lock;
+	std::vector<lock_t *> wait_locks;
+	std::vector<lock_t *> granted_locks;
+	std::vector<lock_t *> new_granted;
+
+	sub_dep_size_total = 0;
+	add_dep_size_total = 0;
+	space = released_lock->un_member.rec_lock.space;
+	page_no = released_lock->un_member.rec_lock.page_no;
+	rec_fold = lock_rec_fold(space, page_no);
+	for (lock = lock_rec_get_first(lock_hash, space, page_no, heap_no);
+		 lock != NULL;
+		 lock = lock_rec_get_next(heap_no, lock)) {
+		if (!lock_get_wait(lock)) {
+			granted_locks.push_back(lock);
+		} else {
+			wait_locks.push_back(lock);
+		}
+	}
+
+	std::sort(wait_locks.begin(), wait_locks.end(), has_higher_priority);
+	for (i = 0; i < wait_locks.size(); ++i) {
+		lock = wait_locks[i];
+		if (!lock_rec_has_to_wait_granted(lock, granted_locks)
+			&& !lock_rec_has_to_wait_granted(lock, new_granted)) {
+			lock_grant(lock);
+			HASH_DELETE(lock_t, hash, lock_hash,
+						rec_fold, lock);
+			lock_rec_insert_to_head(lock_hash, lock, rec_fold);
+			new_granted.push_back(lock);
+			sub_dep_size_total -= lock->trx->dep_size + 1;
+		} else {
+			add_dep_size_total += lock->trx->dep_size + 1;
+		}
+	}
+	if (lock_get_wait(released_lock)) {
+		sub_dep_size_total -= released_lock->trx->dep_size + 1;
+	}
+	for (i = 0; i < granted_locks.size(); ++i) {
+		lock = granted_locks[i];
+		dep_size_compsensate = 0;
+		for (j = 0; j < new_granted.size(); ++j) {
+			new_granted_lock = new_granted[j];
+			if (lock->trx == new_granted_lock->trx) {
+				dep_size_compsensate += lock->trx->dep_size + 1;
+			}
+		}
+		if (lock->trx != released_lock->trx) {
+			update_dep_size(lock->trx, sub_dep_size_total + dep_size_compsensate);
+		}
+	}
+	for (i = 0; i < new_granted.size(); ++i) {
+		lock = new_granted[i];
+		dep_size_compsensate = 0;
+		for (j = 0; j < wait_locks.size(); ++j) {
+			wait_lock = wait_locks[j];
+			if (lock_get_wait(wait_lock)
+				&& lock->trx == wait_lock->trx) {
+				dep_size_compsensate -= lock->trx->dep_size + 1;
+			}
+		}
+		if (lock->trx != released_lock->trx) {
+			update_dep_size(lock->trx, add_dep_size_total + dep_size_compsensate);
+		}
+	}
+}
+
 /*************************************************************//**
 Removes a record lock request, waiting or granted, from the queue and
 grants locks to other transactions in the queue if they now are entitled
 @@ -2439,6 +2752,7 @@ lock_rec_dequeue_from_page(
 {
 	ulint		space;
 	ulint		page_no;
+	ulint		heap_no;
 	lock_t*		lock;
 	trx_lock_t*	trx_lock;
 	hash_table_t*	lock_hash;
 @@ -2465,20 +2779,31 @@ lock_rec_dequeue_from_page(
 	MONITOR_INC(MONITOR_RECLOCK_REMOVED);
 	MONITOR_DEC(MONITOR_NUM_RECLOCK);
 
-	/* Check if waiting locks in the queue can now be granted: grant
-	locks if there are no conflicting locks ahead. Stop at the first
-	X lock that is waiting or has been granted. */
+	if (!use_vats(in_lock->trx)) {
 
-	for (lock = lock_rec_get_first_on_page_addr(lock_hash, space, page_no);
-	     lock != NULL;
-	     lock = lock_rec_get_next_on_page(lock)) {
+		/* Check if waiting locks in the queue can now be granted:
+		 grant locks if there are no conflicting locks ahead. Stop at
+		 the first X lock that is waiting or has been granted. */
 
-		if (lock_get_wait(lock)
-		    && !lock_rec_has_to_wait_in_queue(lock)) {
+		for (lock = lock_rec_get_first_on_page_addr(lock_hash, space,
+																								page_no);
+				 lock != NULL;
+				 lock = lock_rec_get_next_on_page(lock)) {
 
-			/* Grant the lock */
-			ut_ad(lock->trx != in_lock->trx);
-			lock_grant(lock);
+			if (lock_get_wait(lock)
+					&& !lock_rec_has_to_wait_in_queue(lock)) {
+
+				/* Grant the lock */
+				ut_ad(lock->trx != in_lock->trx);
+				lock_grant(lock);
+			}
+		}
+	} else {
+		for (heap_no = 0; heap_no < lock_rec_get_n_bits(in_lock); ++heap_no) {
+			if (!lock_rec_get_nth_bit(in_lock, heap_no)) {
+				continue;
+			}
+			vats_grant(lock_hash, in_lock, heap_no);
 		}
 	}
 }
 @@ -4269,17 +4594,22 @@ lock_rec_unlock(
 	ut_a(!lock_get_wait(lock));
 	lock_rec_reset_nth_bit(lock, heap_no);
 
-	/* Check if we can now grant waiting lock requests */
+	if (!use_vats(trx)) {
 
-	for (lock = first_lock; lock != NULL;
-	     lock = lock_rec_get_next(heap_no, lock)) {
-		if (lock_get_wait(lock)
-		    && !lock_rec_has_to_wait_in_queue(lock)) {
+		/* Check if we can now grant waiting lock requests */
 
-			/* Grant the lock */
-			ut_ad(trx != lock->trx);
-			lock_grant(lock);
+		for (lock = first_lock; lock != NULL;
+				 lock = lock_rec_get_next(heap_no, lock)) {
+			if (lock_get_wait(lock)
+					&& !lock_rec_has_to_wait_in_queue(lock)) {
+
+				/* Grant the lock */
+				ut_ad(trx != lock->trx);
+				lock_grant(lock, false);
+			}
 		}
+	} else {
+		vats_grant(lock_sys->rec_hash, lock, heap_no);
 	}
 
 	lock_mutex_exit();
 @@ -7221,7 +7551,7 @@ DeadlockChecker::get_first_lock(ulint* heap_no) const
 	/* Must find at least two locks, otherwise there cannot be a
 	waiting lock, secondly the first lock cannot be the wait_lock. */
 	ut_a(lock != NULL);
-	ut_a(lock != m_wait_lock);
+	ut_a(lock != m_wait_lock || use_vats(lock->trx));
 
 	/* Check that the lock type doesn't change. */
 	ut_ad(lock_get_type_low(lock) == lock_get_type_low(m_wait_lock));


5、storage/innobase/trx/trx0trx.cc
@@ -1462,6 +1462,9 @@ trx_start_low(
 		trx->start_time = ut_time();
 	}
 
+	trx->dep_size = 0;
+	trx->size_updated = false;
+
 	ut_a(trx->error_state == DB_SUCCESS);


6、storage/innobase/lock/lock0lock.cc
@@ -4605,7 +4605,7 @@ lock_rec_unlock(
 
 				/* Grant the lock */
 				ut_ad(trx != lock->trx);
-				lock_grant(lock, false);
+				lock_grant(lock);